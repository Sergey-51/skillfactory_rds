{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationTask_grid_calculation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4zK9H_Z3zno"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import itertools"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CEJl8rl00fU"
      },
      "source": [
        "В этом ноутбуке предлагается обучить модель классификации изображений датасета CIFAR10, который содержит 60к цветных картинок разрешения 32х32 принадлежащих 10 классам."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVKD8mztW9i4"
      },
      "source": [
        "Загрузим датасет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXyHqQ-x45Wh"
      },
      "source": [
        "def get_load_data(batch_size):\n",
        "  transform_train = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "\n",
        "  transform_test = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "\n",
        "  # batch_size = 512\n",
        "\n",
        "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False, num_workers=2)\n",
        "\n",
        "  classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "  return trainset, trainloader, testset, testloader, classes"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO-hfCP02q79"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def plot_images():\n",
        "  dataiter = iter(trainloader)\n",
        "  images, labels = dataiter.next()\n",
        "\n",
        "  imshow(torchvision.utils.make_grid(images[:4]))\n",
        "  print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9C_wWRm5Ki5"
      },
      "source": [
        "# оригинальная нейронная сеть из задания\n",
        "class Net_org(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size = 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.Tanh()(self.conv1(x)))\n",
        "        x = self.pool(nn.Tanh()(self.conv2(x)))\n",
        "        x = nn.Flatten()(x)\n",
        "        x = nn.Tanh()(self.fc1(x))\n",
        "        x = nn.Tanh()(self.fc2(x))\n",
        "        x = nn.Softmax()(self.fc3(x))\n",
        "        return x"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# изменяю саму модель на AlexNet\n",
        "class Net_AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(256 * 2 * 2, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "        self.drop = nn.Dropout()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.LeakyReLU(0.1)(self.conv1(x)))\n",
        "        x = self.pool(nn.LeakyReLU(0.1)(self.conv2(x)))\n",
        "        x = nn.LeakyReLU(0.1)(self.conv3(x))\n",
        "        x = nn.LeakyReLU(0.1)(self.conv4(x))\n",
        "        x = self.pool(nn.LeakyReLU(0.1)(self.conv5(x)))\n",
        "        # x = nn.Flatten()(x)\n",
        "        x = self.drop(x.view(x.size(0), 256 * 2 * 2))\n",
        "        x = nn.LeakyReLU(0.1)(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = nn.LeakyReLU(0.1)(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qatzWrnwvMkx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создаем список из нескольких нейронных сетей (для выбора лучшей)\n",
        "net_org = Net_org().cuda() # оригинальная сеть из задания\n",
        "net_AlexNet = Net_AlexNet().cuda()\n",
        "\n",
        "# создаем список сетей для прохода циклом по ним\n",
        "nets = [net_org, net_AlexNet]"
      ],
      "metadata": {
        "id": "TBKfTDRtk_o0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ссылка на список официальных функций потерь\n",
        "# https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n",
        "# создам список потерь\n",
        "crit_CrEn = nn.CrossEntropyLoss()\n",
        "# crit_MSELoss=nn.MSELoss()\n",
        "# crit_GaNLL = nn.GaussianNLLLoss()\n",
        "\n",
        "# пока оставил одну функцию для написания логики\n",
        "criterions = [crit_CrEn]#, crit_MSELoss]#, crit_GaNLL]"
      ],
      "metadata": {
        "id": "Glz59vsTuja6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# подозреваю, что так нельзя делать (в этом блоке ниже), тк оптимизаторы получаются привязанными к одной модели\n",
        "# поэтому сделал ниже функцию по активации оптимизаторов в зависимости от модели\n",
        "net=nets[0]\n",
        "opt_Adam = optim.Adam(net.parameters())\n",
        "opt_SGD = optim.SGD(net.parameters(), lr=0.1)\n",
        "opt_Momentum = optim.SGD(net.parameters(),lr=0.1, momentum=0.8)\n",
        "\n",
        "# попробую другие оптимизаторы (их много...)\n",
        "# optimizer = optim.Adadelta(net.parameters())\n",
        "# optimizer = optim.ASGD(net.parameters())\n",
        "# optimizer = optim.NAdam(net.parameters())\n",
        "# optimizer = optim.RAdam(net.parameters())\n",
        " \n",
        "optimizers = [opt_Adam, opt_SGD, opt_Momentum]"
      ],
      "metadata": {
        "id": "-BZypqFdzy36"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizers(net):\n",
        "  # пока пробую на трех оптимизаторах\n",
        "  opt_Adam = optim.Adam(net.parameters())\n",
        "  opt_SGD = optim.SGD(net.parameters(), lr=0.1)\n",
        "  opt_Momentum = optim.SGD(net.parameters(),lr=0.1, momentum=0.8)\n",
        "\n",
        "  # \n",
        "  optimizers = [opt_Adam, opt_SGD, opt_Momentum]\n",
        "  optimizers_names = ['opt_Adam', 'opt_SGD', 'opt_Momentum']\n",
        "  return optimizers, optimizers_names"
      ],
      "metadata": {
        "id": "E8ofzaW06VdR"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK1KB2maMExs"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR4pILtSX91o"
      },
      "source": [
        "Обучим модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFDFu3CV5XQj",
        "outputId": "b3efb4fc-47b5-4f33-8673-bbd0f8b7f81c",
        "cellView": "form",
        "collapsed": true
      },
      "source": [
        "#@title\n",
        "# оригинальный блок обучения\n",
        "for epoch in range(1):  \n",
        "  \n",
        "  for X_batch, y_batch in tqdm(trainloader):\n",
        "      X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      predictions = net(X_batch)\n",
        "\n",
        "      loss = criterion(predictions, y_batch)\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:20<00:00,  4.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# основной блок по обучению и расчета показателя\n",
        "batch_size = 512\n",
        "print('Устанавливаем batch_size=',batch_size)\n",
        "\n",
        "# циклом проходим по всем комбинациям\n",
        "for r in itertools.product(nets, criterions):\n",
        "  torch.cuda.empty_cache() # чистим кэш (у меня вылезала ошибка памяти)\n",
        "\n",
        "  # получаем модель и функцию потерь\n",
        "  net = r[0]\n",
        "  criterion = r[1]\n",
        "\n",
        "  # пересоздаем оптимизаторы под выбранную модель\n",
        "  optimizers, optimizers_names = get_optimizers(net)\n",
        "\n",
        "  # теперь проходим по всем оптимизаторам\n",
        "  for i in range(0,len(optimizers)):\n",
        "    # print(optimizers_names[i])\n",
        "    optimizer = optimizers[i]\n",
        "\n",
        "    print(str(net).split('(')[0],'-',criterion,'-',str(optimizers_names[i]))\n",
        "\n",
        "    # print('Теперь перезагружаем исходные данные')\n",
        "    trainset, trainloader, testset, testloader, classes = get_load_data(batch_size)\n",
        "    # print('Загрузка закончена')\n",
        "\n",
        "    # print('Отрисуем несколько картинок, посмотрим, как выглядит наш датасет')\n",
        "    # plot_images()\n",
        "\n",
        "    # теперь обучаем модель\n",
        "    # эпоху поставил одну просто для ускорения (потом буду изменять, скорее всего на 10)\n",
        "    for epoch in range(1):  \n",
        "      \n",
        "      for X_batch, y_batch in tqdm(trainloader):\n",
        "          X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          predictions = net(X_batch)\n",
        "\n",
        "          loss = criterion(predictions, y_batch)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "    # посчитаем аккуратность обучения\n",
        "    for X_batch, y_batch in testloader:\n",
        "      X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "      with torch.no_grad():\n",
        "          predictions = net(X_batch)\n",
        "\n",
        "      true_lab = y_batch.cpu().numpy()\n",
        "      _, preds = torch.max(predictions, dim=1)\n",
        "      # print(true_lab)\n",
        "      # print(preds)\n",
        "      print('accuracy_score: ', accuracy_score(true_lab, preds.detach().cpu().numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE_QHsdjpfZz",
        "outputId": "70476db1-ff66-4669-9209-9288ceb30d40"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt_Adam\n",
            "Net_org - CrossEntropyLoss() - opt_Adam\n",
            "Устанавливаем batch_size= 512\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:20<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_score:  0.52\n",
            "opt_SGD\n",
            "Net_org - CrossEntropyLoss() - opt_SGD\n",
            "Устанавливаем batch_size= 512\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:20<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_score:  0.5231\n",
            "opt_Momentum\n",
            "Net_org - CrossEntropyLoss() - opt_Momentum\n",
            "Устанавливаем batch_size= 512\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:20<00:00,  4.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_score:  0.4775\n",
            "opt_Adam\n",
            "Net_AlexNet - CrossEntropyLoss() - opt_Adam\n",
            "Устанавливаем batch_size= 512\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:21<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_score:  0.1\n",
            "opt_SGD\n",
            "Net_AlexNet - CrossEntropyLoss() - opt_SGD\n",
            "Устанавливаем batch_size= 512\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:21<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_score:  0.1\n",
            "opt_Momentum\n",
            "Net_AlexNet - CrossEntropyLoss() - opt_Momentum\n",
            "Устанавливаем batch_size= 512\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:21<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_score:  0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запись результатов от ручного перебора  \n",
        "(осталось с предыдущей версии файла)\n",
        "\n",
        "оригинал: accuracy_score:  0.4194  \n",
        "увеличил с 2 -> 5 число эпох: accuracy_score:  0.5182  \n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1) :  accuracy_score:  0.5227  \n",
        "увеличил с 5 -> 10 число эпох: accuracy_score:  0.5468\n",
        "\n",
        "вернул оптимизатор Adam, уменьшил число эпох до 5 и поставил relu как функцию активации: accuracy_score:  0.4219\n",
        "\n",
        "10 эпох, relu и SGD (lr=0.01): accuracy_score:  0.1001  \n",
        "10 эпох, relu и SGD (lr=0.1): accuracy_score: 0.3696\n",
        "\n",
        "10 эпох, relu6 и SGD (lr=0.1): accuracy_score: 0.3596\n",
        "\n",
        "10 эпох, Sigmoid и SGD (lr=0.1):accuracy_score:  0.1\n",
        "\n",
        "посчитал на изначальной модели: accuracy_score:  0.5512\n",
        "\n",
        "посчитал на своей модели: accuracy_score:  0.1089\n",
        "\n",
        "исправил свою модель accuracy_score:  0.6117  \n",
        "увеличил эпохи до 10: accuracy_score:  0.6686  \n",
        "\n",
        "Adam.\n",
        "relu: accuracy_score:  0.7006  \n",
        "Tanh: accuracy_score:  0.6861  \n",
        "Sigmoid: accuracy_score:  0.1  \n",
        "LeakyReLU: accuracy_score:  0.7032  \n",
        "LeakyReLU(0.1) :accuracy_score:  0.7357\n",
        "\n",
        "Теперь попробую другие оптимизаторы для LeakyReLU(0.1):  \n",
        "Adadelta: accuracy_score:  0.6185  \n",
        "ASGD: accuracy_score:  0.1356  \n",
        "NAdam: accuracy_score:  0.3775  \n",
        "RAdam:  accuracy_score:  0.6209  \n",
        "Momentum: accuracy_score:  0.1\n",
        "\n"
      ],
      "metadata": {
        "id": "5e0NfOEbhbDE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbdz7hSeYO2-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}